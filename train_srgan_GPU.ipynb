{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from src.utils import check_GPU_is_available\n",
    "from tensorflow import distribute\n",
    "tf.random.set_seed(69)\n",
    "check_GPU_is_available(is_required=True)\n",
    "strategy = distribute.MirroredStrategy() # Allow multi-GPU training (if available)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4069902689dafafd",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read Training config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4ada854fcf7d778"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "json_cfg_path = \"./GPU_train_cfg.json\"\n",
    "train_cfg = json.load(open(json_cfg_path, 'r'))\n",
    "print(json.dumps(train_cfg, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3720883c26515c49",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Dataloaders"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88427efea674672"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src import create_data_loader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86cf30409d4310d6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# params\n",
    "dataset_tfrecords_path = train_cfg[\"dataset\"][\"TFR_TRAIN_PATH\"]\n",
    "lr_img_shape = train_cfg[\"dataset\"][\"LR_SHAPE\"]\n",
    "hr_img_shape = train_cfg[\"dataset\"][\"HR_SHAPE\"]\n",
    "batch_size = train_cfg[\"trainer\"][\"TRAIN_BATCH_SIZE\"] * strategy.num_replicas_in_sync\n",
    "# deploy\n",
    "assert os.path.exists(dataset_tfrecords_path), print(\"Can't find dataset at the expected directory, please run 'download_dataset.py' first\")\n",
    "train_tfrecords = [os.path.join(dataset_tfrecords_path, x) for x in os.listdir(dataset_tfrecords_path) if \"train\" in x]\n",
    "valid_tfrecords = [os.path.join(dataset_tfrecords_path, x) for x in os.listdir(dataset_tfrecords_path) if \"validation\" in x]\n",
    "train_dataloader = create_data_loader(train_tfrecords, batch_size=batch_size, lr_img_shape=lr_img_shape, hr_img_shape=hr_img_shape, train_mode=True)\n",
    "valid_dataloader = create_data_loader(valid_tfrecords, batch_size=batch_size, lr_img_shape=lr_img_shape, hr_img_shape=hr_img_shape, train_mode=False)\n",
    "print(\"Train tf records: {} - Validation tf records: {}\".format(len(train_tfrecords), len(valid_tfrecords)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7478f60047be1ef4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If a pretrained model is available, load it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70b239ab2b828380"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generator = None\n",
    "pretrained_model_path = \"./assets/pretrained/srgan_gen_div2k-bicubic_x4_1711220278.keras\"\n",
    "\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    from src.model.srgan import SRGAN\n",
    "    from keras import config\n",
    "    with strategy.scope():\n",
    "        config.enable_unsafe_deserialization()\n",
    "        scaling_factor = train_cfg[\"dataset\"][\"SCALING_FACTOR\"]\n",
    "        feature_maps = train_cfg[\"srgan\"][\"FEATURE_MAPS\"]\n",
    "        residual_blocks = train_cfg[\"srgan\"][\"RES_BLOCKS\"]\n",
    "        print(\"[INFO] loading the pretrained SRGAN generator...\")\n",
    "        generator = SRGAN.generator(\n",
    "                scaling_factor=scaling_factor,\n",
    "                feature_maps=feature_maps,\n",
    "                residual_blocks=residual_blocks)\n",
    "        generator.load_weights(pretrained_model_path)\n",
    "        print(\"[INFO] pretrained SRGAN generator loaded!\")\n",
    "else:\n",
    "    print(\"[INFO] pretrained SRGAN generator not found, will train from scratch!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "955dbf4243d46001",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretrain Generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e02bb30041e5e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from src.model.losses import Losses\n",
    "from src.model.srgan import SRGAN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5b876e056bc9d34",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# params\n",
    "scaling_factor = train_cfg[\"dataset\"][\"SCALING_FACTOR\"]\n",
    "feature_maps = train_cfg[\"srgan\"][\"FEATURE_MAPS\"]\n",
    "residual_blocks = train_cfg[\"srgan\"][\"RES_BLOCKS\"]\n",
    "pretrain_lr = train_cfg[\"trainer\"][\"PRETRAIN_LR\"]\n",
    "pretrain_epochs = train_cfg[\"trainer\"][\"PRETRAIN_EPOCHS\"]\n",
    "steps_per_epoch = train_cfg[\"trainer\"][\"STEPS_PER_EPOCH\"]\n",
    "\n",
    "if generator is None:\n",
    "    with strategy.scope():\n",
    "        losses = Losses(numReplicas=strategy.num_replicas_in_sync)\n",
    "        generator = SRGAN.generator(\n",
    "            scaling_factor=scaling_factor,\n",
    "            feature_maps=feature_maps,\n",
    "            residual_blocks=residual_blocks)\n",
    "        generator.compile(\n",
    "            optimizer=Adam(learning_rate=pretrain_lr),\n",
    "            loss=losses.mse_loss)\n",
    "        print(\"[INFO] pretraining SRGAN generator...\")\n",
    "        generator.fit(train_dataloader, \n",
    "                      epochs=pretrain_epochs,\n",
    "                      steps_per_epoch=steps_per_epoch)\n",
    "        \n",
    "        pretrained_base_path = \"./assets/pretrained\"\n",
    "        model_name = \"srgan_gen_\"+train_cfg[\"dataset\"][\"DATASET_NAME\"]+\"_\"+str(int(time.time()))+\".keras\"\n",
    "        pretrained_generator_out_path = os.path.join(pretrained_base_path, model_name)\n",
    "        \n",
    "        if not os.path.exists(pretrained_base_path):\n",
    "            os.makedirs(pretrained_base_path)\n",
    "        \n",
    "        print(f\"[INFO] saving the SRGAN pretrained generator to {pretrained_generator_out_path}...\")\n",
    "        generator.save(pretrained_generator_out_path)\n",
    "        print(\"DONE!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e81d95cb4dedea71",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tune the Generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "373fcb2293b92fd6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.model.vgg import VGG\n",
    "from src.srgan_trainer import SRGANTraining"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb2fa5ffd86dc685",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# params\n",
    "leaky_alpha = train_cfg[\"srgan\"][\"LEAKY_ALPHA\"]\n",
    "disc_blocks = train_cfg[\"srgan\"][\"DISC_BLOCKS\"]\n",
    "finetune_lr = train_cfg[\"trainer\"][\"FINETUNE_LR\"]\n",
    "finetune_epochs = train_cfg[\"trainer\"][\"FINETUNE_EPOCHS\"]\n",
    "\n",
    "with strategy.scope():\n",
    "    losses = Losses(numReplicas=strategy.num_replicas_in_sync)\n",
    "    vgg = VGG.build()\n",
    "    discriminator = SRGAN.discriminator(\n",
    "        feature_maps=feature_maps, \n",
    "        leaky_alpha=leaky_alpha, \n",
    "        disc_blocks=disc_blocks)\n",
    "    srgan = SRGANTraining(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        vgg=vgg,\n",
    "        batch_size=batch_size)\n",
    "    srgan.compile(\n",
    "        d_optimizer=Adam(learning_rate=finetune_lr),\n",
    "        g_optimizer=Adam(learning_rate=finetune_lr),\n",
    "        bce_loss=losses.bce_loss,\n",
    "        mse_loss=losses.mse_loss,\n",
    "    )\n",
    "    print(\"[INFO] fine-tuning SRGAN...\")\n",
    "    srgan.fit(train_dataloader, \n",
    "              epochs=finetune_epochs,\n",
    "              steps_per_epoch=steps_per_epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a453daf378d38811",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "finetuned_base_path = \"./assets/finetuned\"\n",
    "model_name = \"srgan_gen_\"+train_cfg[\"dataset\"][\"DATASET_NAME\"]+\"_\"+str(int(time.time()))+\"_finetuned.keras\"\n",
    "finetuned_generator_out_path = os.path.join(finetuned_base_path, model_name)\n",
    "\n",
    "if not os.path.exists(finetuned_base_path):\n",
    "    os.makedirs(finetuned_base_path)\n",
    "\n",
    "print(f\"[INFO] saving the SRGAN finetuned generator to {finetuned_generator_out_path}...\")\n",
    "generator.save(finetuned_generator_out_path)\n",
    "print(\"DONE!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4c35069b93731f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d1a105d2f837811"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
